{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6222d1d0-03f3-4250-8b11-72d7c110aed9",
   "metadata": {},
   "source": [
    "# Retrieval Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93e3c4f-9e6f-40fd-8167-5be7b7fd1b61",
   "metadata": {},
   "source": [
    "replicated from langchain, https://python.langchain.com/docs/get_started/quickstart#conversation-retrieval-chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16a255e-056e-4a05-acf3-36b2eff3f02a",
   "metadata": {},
   "source": [
    "## initializing Openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1931f901-0f09-4488-bc5a-91cbc0f7b1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce969880-12c8-4d90-a516-0fa60c738bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e62f6a-38d3-4b0f-b8d2-7322a35d4dae",
   "metadata": {},
   "source": [
    "## loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae26381e-7aac-41c0-b860-bc86dac0e910",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com/overview\")\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d917d3d0-7db6-4073-80da-b5d31264d5ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(docs),len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c3555c-16d3-4e2a-8b75-09db8690f2d3",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "601f5329-6f97-467f-bb2c-224180513fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a3f4eb-e7bd-441b-ba99-48d7f968d944",
   "metadata": {},
   "source": [
    "## Using local vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6553b250-f44b-456c-9478-88c523c2ef3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# splitting documents\n",
    "text_splitter = RecursiveCharacterTextSplitter()\n",
    "documents = text_splitter.split_documents(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "01d65d2a-ec78-44f3-ad1a-bfc97aeaff4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "000329d0-b198-497e-977f-3fe52a1ef041",
   "metadata": {},
   "outputs": [],
   "source": [
    "#keeping them as embeddings in a vector store, vector, with an index\n",
    "vector = FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce4ba5ea-c825-41a4-aca3-3e02c6e085f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_community.vectorstores.faiss.FAISS"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152feb7b-a9cd-41ac-9082-2f460ef47b8a",
   "metadata": {},
   "source": [
    "## Template including specific context and chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7986ec67-3c24-439d-ba3c-252af8f6b629",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"Answer the following question based only on the provided context:\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {input}\"\"\")\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b9c26296-713e-4aff-ac02-2428c9d7d83a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.runnables.base.RunnableBinding"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a5da7778-e372-4a2b-946b-ca912c8c60f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# document_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3a3361-be6a-4067-88a4-ca14e3b010ec",
   "metadata": {},
   "source": [
    "## build retriever from vector store ,and add it to the chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6fcc6864-f610-4100-8ac1-68dd136f109f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "retriever = vector.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "93a8728f-bf66-4ed0-afd6-b88833d74700",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = retrieval_chain.invoke({\"input\": \"how can langsmith help with testing?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0da8ae1d-b86e-486b-940f-829d9731e3bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input', 'context', 'answer'])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b506b735-0d17-4743-98e8-1a171c884d7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(response['context'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "94c3634f-a476-456a-a40b-8eea4381c8aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith can help with testing by simplifying dataset uploading and providing tools to run chains over data points and visualize the outputs. The LangSmith client allows users to pull down a dataset and run a chain over them, logging the results to a new project associated with the dataset. Users can review the results, assign feedback to runs, and mark them as correct or incorrect directly in the web app. LangSmith also offers a set of evaluators that can be specified when initiating a test run, providing guidance on examples that should be looked at. Additionally, LangSmith facilitates human evaluation through annotation queues, allowing users to manually review and annotate runs for subjective qualities or to validate auto-evaluated runs.\n"
     ]
    }
   ],
   "source": [
    "print(response['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c833a41b-c6cf-4211-81de-a657110dcba5",
   "metadata": {},
   "source": [
    "## Conversational Retrieval chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ea1c98b2-8ad3-4b1a-bb9e-902663c648c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "# First we need a prompt that we can pass into an LLM to generate this search query\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "    (\"user\", \"Given the above conversation, generate a search query to look up in order to get information relevant to the conversation\")\n",
    "])\n",
    "retriever_chain = create_history_aware_retriever(llm, retriever, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04d6bae-fdf1-4764-a806-e78a892461b2",
   "metadata": {},
   "source": [
    "### dummy test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fa768375-4556-4581-af57-e3542a11be6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "chat_history = [HumanMessage(content=\"Can LangSmith help test my LLM applications?\"), AIMessage(content=\"Yes!\")]\n",
    "response =  retriever_chain.invoke({\n",
    "    \"chat_history\": chat_history,\n",
    "    \"input\": \"Tell me how\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b871018d-2b78-4b8f-a8dc-a31ba848d094",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(response) # all are documents, so the retriever chain outputs 4 documents\n",
    "# we will now ask out fo those 4 documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f96db217-5081-40d4-8069-91ed66aa5ba7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='LangSmith Overview and User Guide | ü¶úÔ∏èüõ†Ô∏è LangSmith', metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7e8777-bacd-4026-b02e-894ff59bd29e",
   "metadata": {},
   "source": [
    "### chaining document chain and retriver_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c5d43583-2dac-4ef9-990a-0fb5ac043b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Answer the user's questions based on the below context:\\n\\n{context}\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "])\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "retrieval_chain = create_retrieval_chain(retriever_chain, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f7a5919c-1b0e-4b9e-a575-abcc69d16171",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = [HumanMessage(content=\"Can LangSmith help test my LLM applications?\"), AIMessage(content=\"Yes!\")]\n",
    "response_combined_chain = retrieval_chain.invoke({\n",
    "    \"chat_history\": chat_history,\n",
    "    \"input\": \"Tell me how\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "516fef3a-36e8-4c1e-8c10-fdd1b64bded7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response_combined_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a6e04851-1586-4c39-8022-e459418a4e4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['chat_history', 'input', 'context', 'answer'])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_combined_chain.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "39659da2-0b07-43f5-9022-005d780aec3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Can LangSmith help test my LLM applications?'),\n",
       " AIMessage(content='Yes!')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_combined_chain['chat_history']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0834a234-9d46-4519-95b2-4d51d236205b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the documents\n",
    "# response_combined_chain['context'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8763270d-7c26-409e-a190-6c42e26ad33d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tell me how'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_combined_chain['input']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9a08d6ee-1e71-4ef4-8095-cee8d8705eea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangSmith can help test your LLM applications in a few ways:\\n\\n1. Dataset Testing: Once you have a dataset, you can use LangSmith to run your LLM chain over the data points and visualize the outputs. This can be done using the LangSmith client, which makes it easy to pull down a dataset and run a chain over them. You can log the results to a new project associated with the dataset and review them. You can also assign feedback to runs and mark them as correct or incorrect directly in the web app, displaying aggregate statistics for each test project.\\n\\n2. Evaluators: LangSmith provides a set of evaluators in the open-source LangChain library. These evaluators can be specified when initiating a test run and will evaluate the results once the test run completes. While these evaluators may not be perfect, they can guide your eye to examples that you should look at, especially when dealing with a large number of data points.\\n\\n3. Human Evaluation: LangSmith makes it easy to manually review and annotate runs through annotation queues. You can select runs based on criteria like model type or automatic evaluation scores and queue them up for human review. As a reviewer, you can quickly step through the runs, view the input, output, and existing tags, and add your own feedback. This is useful for assessing subjective qualities that automatic evaluators struggle with, such as creativity or humor, and validating the accuracy of automatic metrics.\\n\\nBy utilizing these testing features in LangSmith, you can ensure the quality and reliability of your LLM applications.'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_combined_chain['answer']"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7ae8afd5-36fb-4c81-924e-c8a33522c27f",
   "metadata": {},
   "source": [
    "Indeed much better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4caaa19f-2870-4de5-a575-3ddc0bbe1530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(retriever.get_relevant_documents(\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d664ca-466a-4447-8ddc-0ff0258f29b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
